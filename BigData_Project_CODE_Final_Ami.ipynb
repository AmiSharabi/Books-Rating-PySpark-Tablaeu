{"cells":[{"cell_type":"markdown","metadata":{"id":"gdH6RLqfewQg"},"source":["Big Data Project- Ami\n","\n","> Indented block\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ghn33L2pvsKO"},"source":["# Contents\n","\n","1. Connecting to Spark Agent & to drive in order to uplaod files, creating pathways for files\n","\n","2. Preparing Books Dataframe & creating the DIM version\n","\n","3. preparing Users Dataframe & creating the DIM version\n","\n","4. Preparing Rating Dataframe & creating FACT version\n","\n","5. Writing new DIMS & FACT to CSV format\n","\n","6. Extras"]},{"cell_type":"markdown","metadata":{"id":"j76kQs1nwlmP"},"source":["##1. Connection!"]},{"cell_type":"code","source":["# innstall java\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# install spark (change the version number if needed)\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n","\n","# unzip the spark file to the current folder\n","!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n","\n","# set your spark folder to your system path environment.\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n","\n","\n","# install findspark using pip\n","!pip install -q findspark\n"],"metadata":{"id":"1DKfpeGM2P0Y","executionInfo":{"status":"ok","timestamp":1689784429095,"user_tz":-180,"elapsed":53034,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip install pyspark==3.0.0\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","from pyspark.sql import Row\n","from pyspark.sql import functions as f"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDTdbQ3I7M0R","executionInfo":{"status":"ok","timestamp":1689784470543,"user_tz":-180,"elapsed":41459,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}},"outputId":"eabd9fb7-6238-45b1-b989-ad637fec8b81"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark==3.0.0\n","  Downloading pyspark-3.0.0.tar.gz (204.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.7/204.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9 (from pyspark==3.0.0)\n","  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044167 sha256=df557454be197165c7ef09a64565f7cdb3d57a201cdf1da5da2e94325afb17a3\n","  Stored in directory: /root/.cache/pip/wheels/b1/bb/8b/ca24d3f756f2ed967225b0871898869db676eb5846df5adc56\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","  Attempting uninstall: py4j\n","    Found existing installation: py4j 0.10.9.7\n","    Uninstalling py4j-0.10.9.7:\n","      Successfully uninstalled py4j-0.10.9.7\n","Successfully installed py4j-0.10.9 pyspark-3.0.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAlyZzubfSCy","outputId":"e1b78bbe-7061-4646-cef6-10185ffe991d","executionInfo":{"status":"ok","timestamp":1689784476464,"user_tz":-180,"elapsed":5933,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting isbnlib\n","  Downloading isbnlib-3.10.14-py2.py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: isbnlib\n","Successfully installed isbnlib-3.10.14\n"]}],"source":["#Installing a library that confirms an ISBN as valid\n","\n","!pip install isbnlib\n","\n","from isbnlib import is_isbn10, is_isbn13\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7ArRPcjfxtg","outputId":"4ef003a7-e904-4a2c-cd36-e68d5e511e9d","executionInfo":{"status":"ok","timestamp":1689784496699,"user_tz":-180,"elapsed":20244,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\n","#Connecting to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"e49jepQpgGud","executionInfo":{"status":"ok","timestamp":1689784496699,"user_tz":-180,"elapsed":7,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}}},"outputs":[],"source":["#Setting Up personal Folder Paths' Data Files reside (Remove '#' where relevant)\n","\n","\n","#Ami's File location:\n","FolderPath =  '/content/drive/Othercomputers/Ami computer /Ami/Technion/4.big data/Big Data Project/data'"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"eaxKmFp22oIR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689784498996,"user_tz":-180,"elapsed":2303,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}},"outputId":"62fbb981-b07d-4151-f667-f8aec342b70d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"uVUonM5VhNGf","executionInfo":{"status":"ok","timestamp":1689784519412,"user_tz":-180,"elapsed":20418,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}}},"outputs":[],"source":["# reading files\n","\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n","\n","#uploaded in standard format\n","BookRatings_DF = spark.read.csv(FolderPath + '/BX-Book-Ratings.csv', header = True, inferSchema= True)\n","\n","#Best way to upload file without losing data, parsing via a delimiter before reading\n","Books_DF = (spark.read.option(\"delimiter\", ';\"').option(\"header\", True).format(\"csv\").load(FolderPath + '/BX-Books.csv'))\n","\n","#Users CSV is missing headers, creating schema with headers before the read\n","schema = StructType([\n","    StructField(\"UserstoSPLIT\", StringType(), True),\n","    StructField(\"State\", StringType(), True),\n","    StructField(\"AgeSPLIT\", StringType(), True)])\n","\n","Users_DF = spark.read.csv(FolderPath + '/BX-Users.csv', header = True, schema = schema) # Using schema made above\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"kwJoZdOliEJQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689784521382,"user_tz":-180,"elapsed":1981,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}},"outputId":"3190891d-acc8-4b45-e7e1-67d9fe7de7e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------------------------+----+----+\n","|\"User-ID;\"\"ISBN\"\";\"\"Book-Rating\"\"\"|_c1 |_c2 |\n","+----------------------------------+----+----+\n","|\"276725;\"\"034545104X\"\";\"\"0\"\"\"     |null|null|\n","|\"276726;\"\"0155061224\"\";\"\"5\"\"\"     |null|null|\n","|\"276727;\"\"0446520802\"\";\"\"0\"\"\"     |null|null|\n","|\"276729;\"\"052165615X\"\";\"\"3\"\"\"     |null|null|\n","|\"276729;\"\"0521795028\"\";\"\"6\"\"\"     |null|null|\n","|\"276733;\"\"2080674722\"\";\"\"0\"\"\"     |null|null|\n","|\"276736;\"\"3257224281\"\";\"\"8\"\"\"     |null|null|\n","|\"276737;\"\"0600570967\"\";\"\"6\"\"\"     |null|null|\n","|\"276744;\"\"038550120X\"\";\"\"7\"\"\"     |null|null|\n","|\"276745;\"\"342310538\"\";\"\"10\"\"\"     |null|null|\n","|\"276746;\"\"0425115801\"\";\"\"0\"\"\"     |null|null|\n","|\"276746;\"\"0449006522\"\";\"\"0\"\"\"     |null|null|\n","|\"276746;\"\"0553561618\"\";\"\"0\"\"\"     |null|null|\n","|\"276746;\"\"055356451X\"\";\"\"0\"\"\"     |null|null|\n","|\"276746;\"\"0786013990\"\";\"\"0\"\"\"     |null|null|\n","|\"276746;\"\"0786014512\"\";\"\"0\"\"\"     |null|null|\n","|\"276747;\"\"0060517794\"\";\"\"9\"\"\"     |null|null|\n","|\"276747;\"\"0451192001\"\";\"\"0\"\"\"     |null|null|\n","|\"276747;\"\"0609801279\"\";\"\"0\"\"\"     |null|null|\n","|\"276747;\"\"0671537458\"\";\"\"9\"\"\"     |null|null|\n","+----------------------------------+----+----+\n","only showing top 20 rows\n","\n","+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|      ISBN|         Book-Title\"|        Book-Author\"|Year-Of-Publication\"|          Publisher\"|        Image-URL-S\"|        Image-URL-M\"|       Image-URL-L\"\"|\n","+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|0195153448|Classical Mythology\"| Mark P. O. Morford\"|               2002\"|Oxford University...|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0002005018|       Clara Callan\"|Richard Bruce Wri...|               2001\"|HarperFlamingo Ca...|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0060973129|Decision in Norma...|       Carlo D'Este\"|               1991\"|    HarperPerennial\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0374157065|Flu: The Story of...|   Gina Bari Kolata\"|               1999\"|Farrar Straus Gir...|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0393045218|The Mummies of Ur...|    E. J. W. Barber\"|               1999\"|W. W. Norton &amp...|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0399135782|The Kitchen God's...|            Amy Tan\"|               1991\"|   Putnam Pub Group\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0425176428|What If?: The Wor...|      Robert Cowley\"|               2000\"|Berkley Publishin...|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0671870432|    PLEADING GUILTY\"|        Scott Turow\"|               1993\"|         Audioworks\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0679425608|Under the Black F...|    David Cordingly\"|               1996\"|       Random House\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","|074322678X|Where You'll Find...|        Ann Beattie\"|               2002\"|           Scribner\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0771074670|Nights Below Stat...|David Adams Richa...|               1988\"|    Emblem Editions\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","|080652121X|Hitler's Secret B...|         Adam Lebor\"|               2000\"|      Citadel Press\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0887841740| The Middle Stories\"|        Sheila Heti\"|               2004\"|House of Anansi P...|http://images.ama...|http://images.ama...|http://images.ama...|\n","|1552041778|           Jane Doe\"|       R. J. Kaiser\"|               1999\"|         Mira Books\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","|1558746218|A Second Chicken ...|      Jack Canfield\"|               1998\"|Health Communicat...|http://images.ama...|http://images.ama...|http://images.ama...|\n","|1567407781|The Witchfinder (...|  Loren D. Estleman\"|               1998\"|Brilliance Audio ...|http://images.ama...|http://images.ama...|http://images.ama...|\n","|1575663937|More Cunning Than...| Robert Hendrickson\"|               1999\"|Kensington Publis...|http://images.ama...|http://images.ama...|http://images.ama...|\n","|1881320189|Goodbye to the Bu...|       Julia Oliver\"|               1994\"|     River City Pub\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0440234743|      The Testament\"|       John Grisham\"|               1999\"|               Dell\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","|0452264464|Beloved (Plume Co...|      Toni Morrison\"|               1994\"|              Plume\"|http://images.ama...|http://images.ama...|http://images.ama...|\n","+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 20 rows\n","\n","+--------------------+----------------+--------------------+\n","|        UserstoSPLIT|           State|            AgeSPLIT|\n","+--------------------+----------------+--------------------+\n","|            \"1\";\"nyc|        new york|           usa\";NULL|\n","|       \"2\";\"stockton|      california|           usa\";\"18\"|\n","|         \"3\";\"moscow| yukon territory|        russia\";NULL|\n","|          \"4\";\"porto|        v.n.gaia|      portugal\";\"17\"|\n","|    \"5\";\"farnborough|           hants| united kingdom\";...|\n","|   \"6\";\"santa monica|      california|           usa\";\"61\"|\n","|     \"7\";\"washington|              dc|           usa\";NULL|\n","|        \"8\";\"timmins|         ontario|        canada\";NULL|\n","|     \"9\";\"germantown|       tennessee|           usa\";NULL|\n","|      \"10\";\"albacete|       wisconsin|         spain\";\"26\"|\n","|     \"11\";\"melbourne|        victoria|     australia\";\"14\"|\n","|    \"12\";\"fort bragg|      california|           usa\";NULL|\n","|     \"13\";\"barcelona|       barcelona|         spain\";\"26\"|\n","|    \"14\";\"mediapolis|            iowa|           usa\";NULL|\n","|       \"15\";\"calgary|         alberta|        canada\";NULL|\n","|   \"16\";\"albuquerque|      new mexico|           usa\";NULL|\n","|    \"17\";\"chesapeake|        virginia|           usa\";NULL|\n","|\"18\";\"rio de janeiro|  rio de janeiro|        brazil\";\"25\"|\n","|        \"19\";\"weston|                |              \";\"14\"|\n","|     \"20\";\"langhorne|    pennsylvania|           usa\";\"19\"|\n","+--------------------+----------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["#Lets Have a look at the mess\n","BookRatings_DF.show(truncate = False)\n","Books_DF.show()\n","Users_DF.show()\n"]},{"cell_type":"markdown","metadata":{"id":"7ko7T8rSvl1z"},"source":["##2. Preparing Book Dataframe"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"EyOxpBLB6wo5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689784683648,"user_tz":-180,"elapsed":162272,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}},"outputId":"77761a61-45bd-4ba5-c221-a0378b4b03d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Null Count Before Clean\n","+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n","|ISBN|Book_Title|Book_Author|Year_Of_Publication|Publisher|Image_URL_S|Image_URL_M|Image_URL_L|\n","+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n","|   0|         0|         41|                  7|        0|        100|          0|          0|\n","+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n","\n","Null Count after Clean\n","+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n","|ISBN|Book_Title|Book_Author|Year_Of_Publication|Publisher|Image_URL_S|Image_URL_M|Image_URL_L|\n","+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n","|   0|         0|          0|                  0|        0|          0|          0|          0|\n","+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n","\n","After filling empty values as unknown, we count the database again\n","Unknown Count\n","+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n","|ISBN|Book_Title|Book_Author|Year_Of_Publication|Publisher|Image_URL_S|Image_URL_M|Image_URL_L|\n","+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n","|   0|         0|         84|                  7|        1|        100|          0|          0|\n","+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n","\n","Count of Years with improbable values 0-1000\n","+-------+--------------------+-------------------------------+-----------+-------------------+--------------------------+-------------------------------------------------+-------------------------------------------------+-------------------------------------------------+\n","|summary|ISBN                |Book_Title                     |Book_Author|Year_Of_Publication|Publisher                 |Image_URL_S                                      |Image_URL_M                                      |Image_URL_L                                      |\n","+-------+--------------------+-------------------------------+-----------+-------------------+--------------------------+-------------------------------------------------+-------------------------------------------------+-------------------------------------------------+\n","|count  |4619                |4619                           |4619       |4619               |4619                      |4619                                             |4619                                             |4619                                             |\n","|mean   |1.6863833723714218E9|1407.0                         |null       |0.0                |null                      |null                                             |null                                             |null                                             |\n","|stddev |2.2051678338192625E9|999.3933159672422              |null       |0.0                |null                      |null                                             |null                                             |null                                             |\n","|min    |0001046438          |006781 bk1 gags de boule et bil|a a milne  |0                  |a s barnes and company inc|httpimagesamazoncomimagesp000104643801thumbzzzjpg|httpimagesamazoncomimagesp000104643801mzzzzzzzjpg|httpimagesamazoncomimagesp000104643801lzzzzzzzjpg|\n","|max    |b0002k6k8o          |zyeux bleus                    |zyke       |0                  |zeropanik press           |unknown                                          |httpimagesamazoncomimagespb0002k6k8o01mzzzzzzzjpg|httpimagesamazoncomimagespb0002k6k8o01lzzzzzzzjpg|\n","+-------+--------------------+-------------------------------+-----------+-------------------+--------------------------+-------------------------------------------------+-------------------------------------------------+-------------------------------------------------+\n","\n","#########################################################################################################################\n","After Running our state of the art high tech algorythim, we now know how many bookshave valid and invalid ISBN's\n","\n","There are 271262 Books with Valid ISBN\n","There are 117 Books with Invalid ISBN\n","\n","\n","There should be 271262 books with valid ISBN in the updated database\n","\n","Here is a count of the total rows in the updated Books dataframe\n","\n","#####  271262  #####\n","Creating DIM_BOOKS...........\n","\n","\n","Final Look at the DIM\n","+------+----------+--------------------+--------------------+-------------------+--------------------+\n","|BookSK|    BookBK|          Book_Title|         Book_Author|Year_Of_Publication|           Publisher|\n","+------+----------+--------------------+--------------------+-------------------+--------------------+\n","|   101|0195153448| classical mythology|    mark p o morford|               2002|oxford university...|\n","|   102|0002005018|        clara callan|richard bruce wright|               2001|harperflamingo ca...|\n","|   103|0060973129|decision in normandy|         carlo deste|               1991|     harperperennial|\n","|   104|0374157065|flu the story of ...|    gina bari kolata|               1999|farrar straus giroux|\n","|   105|0393045218|the mummies of ur...|        e j w barber|               1999|w w norton amp co...|\n","|   106|0399135782|the kitchen gods ...|             amy tan|               1991|    putnam pub group|\n","|   107|0425176428|what if the world...|       robert cowley|               2000|berkley publishin...|\n","|   108|0671870432|     pleading guilty|         scott turow|               1993|          audioworks|\n","|   109|0679425608|under the black f...|     david cordingly|               1996|        random house|\n","|   110|074322678x|where youll find ...|         ann beattie|               2002|            scribner|\n","|   111|0771074670|nights below stat...|david adams richards|               1988|     emblem editions|\n","|   112|080652121x|hitlers secret ba...|          adam lebor|               2000|       citadel press|\n","|   113|0887841740|  the middle stories|         sheila heti|               2004|house of anansi p...|\n","|   114|1552041778|            jane doe|          r j kaiser|               1999|          mira books|\n","|   115|1558746218|a second chicken ...|       jack canfield|               1998|health communicat...|\n","|   116|1567407781|the witchfinder a...|    loren d estleman|               1998|brilliance audio ...|\n","|   117|1575663937|more cunning than...|  robert hendrickson|               1999|kensington publis...|\n","|   118|1881320189|goodbye to the bu...|        julia oliver|               1994|      river city pub|\n","|   119|0440234743|       the testament|        john grisham|               1999|                dell|\n","|   120|0452264464|beloved plume con...|       toni morrison|               1994|               plume|\n","+------+----------+--------------------+--------------------+-------------------+--------------------+\n","only showing top 20 rows\n","\n","+-------+----------------+--------------------+--------------------+--------------------+-------------------+----------------+\n","|summary|          BookSK|              BookBK|          Book_Title|         Book_Author|Year_Of_Publication|       Publisher|\n","+-------+----------------+--------------------+--------------------+--------------------+-------------------+----------------+\n","|  count|          271262|              271262|              271262|              271262|             271262|          271262|\n","|   mean|        135731.5|1.0412353852604752E9|            Infinity|                null| 1993.6922557659855|            null|\n","| stddev|78306.7386978924|1.4877523499740462E9|                 NaN|                null|  8.318529693933133|            null|\n","|    min|             101|          0000913154|$14 in the bank c...|142 moms from all...|               1376|      1001 nuits|\n","|    max|          271362|          9999999999|zzzzzzzz mnima de...|           zz packer|            unknown|zzdap publishing|\n","+-------+----------------+--------------------+--------------------+--------------------+-------------------+----------------+\n","\n"]}],"source":["#Cleaning Books DataFrame\n","\n","#More Libraries\n","import re\n","from pyspark.sql.types import StringType\n","from pyspark.sql.types import IntegerType\n","from pyspark.sql.functions import col,isnan,when,count\n","\n","\n","\n","# IMPORTANT, special character '&' created a delimiter in wrong place, this little fix saved 5000+ rows of data!\n","\n","for C1 in Books_DF.columns:\n","  BooksDF_1 = Books_DF.withColumn(C1, f.regexp_replace(C1, '&amp;', 'and'))\n","\n","\n","\n","#Getting rid of junk characters\n","for C1 in BooksDF_1.columns:\n","  BooksDF_1 = BooksDF_1.withColumn(C1, f.regexp_replace(C1, '[^0-9a-zA-Z $]+', ''))\n","\n","BooksDF_1 = BooksDF_1.select([f.col(col).alias(re.sub(\"-\",\"_\",col)) for col in BooksDF_1.columns])\n","\n","BooksDF_1 = BooksDF_1.select([f.col(col).alias(re.sub(\"[^0-9a-zA-Z_$]+\",\"\",col)) for col in BooksDF_1.columns])\n","\n","#Trimming\n","from pyspark.sql.functions import trim\n","for C1 in BooksDF_1.columns:\n","  BooksDF_1 = BooksDF_1.withColumn(C1, trim(col(C1)))\n","\n","#Getting rid of empty values\n","for c1 in BooksDF_1.columns:\n","  BooksDF_1 = BooksDF_1.withColumn(c1, when(col(c1) == '', 'unknown').otherwise(col(c1)))\n","\n","#Finding out how many nulls in dataset\n","#for some reason only works with import before code\n","print('Null Count Before Clean')\n","from pyspark.sql.functions import col,isnan,when,count\n","BooksDF_1.select([count(when(isnan(C1) | col(C1).isNull(), C1)).alias(C1) for C1 in BooksDF_1.columns]\n","   ).show()\n","\n","#Fill Nulls with 'Unkown' because values are qualitative\n","BooksDF_1 = BooksDF_1.fillna('Unknown')\n","\n","#No Nulls\n","print('Null Count after Clean')\n","from pyspark.sql.functions import col,isnan,when,count\n","BooksDF_1.select([count(when(isnan(C1) | col(C1).isNull(), C1)).alias(C1) for C1 in BooksDF_1.columns]\n","   ).show()\n","\n","#Lowecase All\n","for C1 in BooksDF_1.columns:\n","    BooksDF_1 = BooksDF_1.withColumn(C1, f.lower(f.col(C1)))\n","\n","\n","#Check Nulls are filled with unknown\n","print('After filling empty values as unknown, we count the database again')\n","print('Unknown Count')\n","from pyspark.sql.functions import col,isnan,when,count\n","BooksDF_1.select([count(when(col(C1)== 'unknown', C1)).alias(C1) for C1 in BooksDF_1.columns]\n","   ).show()\n","\n","BooksforFilter = BooksDF_1\n","\n","# 4000 rows have '0' as publish year\n","Filter= BooksforFilter.filter( \\\n","    col(\"Year_Of_Publication\") < 1000 \\\n","  )\n","print('Count of Years with improbable values 0-1000')\n","Filter.describe().show(truncate=False)\n","\n","#Turning unusual years to unknown\n","BooksDF_1 = BooksDF_1.withColumn('Year_Of_Publication', when((col('Year_Of_Publication') < 200) | \\\n","                                                              (~col('Year_Of_Publication').rlike(\"^[0-9]*$\")), 'unknown') \\\n","                                                             .otherwise(col('Year_Of_Publication')))\n","\n","#Columns which have only numeric values but should be limited to alphabetic, changed to 'unknown'\n","for C1 in BooksDF_1.columns:\n","  if C1 not in ('ISBN', 'Year_Of_Publication'):\n","    BooksDF_1 = BooksDF_1.withColumn(C1, when(col(C1).rlike(\"^[0-9]*$\"), 'unknown').otherwise(col(C1)))\n","\n","\n","####################################################################################################################################################\n","\n","# ISBN VALIDITY SCREANING\n","\n","####################################################################################################################################################\n","\n","ValidISBN = []\n","InvalidISBN = []\n","\n","row_listBOOKS = BooksDF_1.collect()\n","\n","for i in row_listBOOKS:\n","  if (is_isbn10(i.__getitem__('ISBN')) == True):\n","    ValidISBN.append(i)\n","  else:\n","    InvalidISBN.append(i)\n","print('#########################################################################################################################')\n","print(\"After Running our state of the art high tech algorythim, we now know how many bookshave valid and invalid ISBN's\")\n","print('')\n","print(f'There are {len(ValidISBN)} Books with Valid ISBN')\n","print(f'There are {len(InvalidISBN)} Books with Invalid ISBN')\n","\n","InvalidISBN_ONLY = []\n","for i in InvalidISBN:\n","  InvalidISBN_ONLY.append(i.__getitem__('ISBN'))\n","\n","BooksDF_1 = BooksDF_1.filter(~col('ISBN').isin(InvalidISBN_ONLY))\n","print('')\n","print('')\n","print(f'There should be {len(ValidISBN)} books with valid ISBN in the updated database')\n","print('')\n","\n","print('Here is a count of the total rows in the updated Books dataframe')\n","print('')\n","print('#####  ' + f'{BooksDF_1.count()}' + '  #####' )\n","\n","#CREATING DIM_BOOKS\n","\n","print('Creating DIM_BOOKS...........')\n","\n","BooksDF_1 = BooksDF_1.drop('Image_URL_S','Image_URL_M','Image_URL_L')\n","\n","BooksDF_1 = BooksDF_1.withColumnRenamed('ISBN', 'BookBK')\n","print('')\n","print('')\n","\n","from pyspark.sql.functions import row_number, monotonically_increasing_id\n","from pyspark.sql import Window\n","\n","BooksDF_1 = BooksDF_1.withColumn(\n","    \"BookSK\",\n","    row_number().over(Window.orderBy(monotonically_increasing_id()))+100\n",")\n","\n","BooksDF_1 = BooksDF_1.select('BookSK', 'BookBK', 'Book_Title', 'Book_Author', 'Year_Of_Publication', 'Publisher')\n","\n","DIM_BOOKS = BooksDF_1\n","#Se Tout\n","print('Final Look at the DIM')\n","\n","\n","DIM_BOOKS.show()\n","DIM_BOOKS.describe().show()\n"]},{"cell_type":"markdown","metadata":{"id":"RPLppLe_u-xn"},"source":["##3. Preparing Users Dataframe"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"wB-rJFVW7R9t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689784962546,"user_tz":-180,"elapsed":278908,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}},"outputId":"e1afb852-2838-44be-acbf-90f2ef05f7f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["UsersDF_1 before transformation\n","+-------+--------------+---------------+--------------+---+\n","|User_ID|City          |State          |Country       |Age|\n","+-------+--------------+---------------+--------------+---+\n","|1      |nyc           |new york       |usa           |   |\n","|2      |stockton      |california     |usa           |18 |\n","|3      |moscow        |yukon territory|russia        |   |\n","|4      |porto         |vngaia         |portugal      |17 |\n","|5      |farnborough   |hants          |united kingdom|   |\n","|6      |santa monica  |california     |usa           |61 |\n","|7      |washington    |dc             |usa           |   |\n","|8      |timmins       |ontario        |canada        |   |\n","|9      |germantown    |tennessee      |usa           |   |\n","|10     |albacete      |wisconsin      |spain         |26 |\n","|11     |melbourne     |victoria       |australia     |14 |\n","|12     |fort bragg    |california     |usa           |   |\n","|13     |barcelona     |barcelona      |spain         |26 |\n","|14     |mediapolis    |iowa           |usa           |   |\n","|15     |calgary       |alberta        |canada        |   |\n","|16     |albuquerque   |new mexico     |usa           |   |\n","|17     |chesapeake    |virginia       |usa           |   |\n","|18     |rio de janeiro|rio de janeiro |brazil        |25 |\n","|19     |weston        |               |              |14 |\n","|20     |langhorne     |pennsylvania   |usa           |19 |\n","+-------+--------------+---------------+--------------+---+\n","only showing top 20 rows\n","\n","Null count before clean\n","+-------+----+-----+-------+------+\n","|User_ID|City|State|Country|   Age|\n","+-------+----+-----+-------+------+\n","|      1| 706|17392|   4741|111715|\n","+-------+----+-----+-------+------+\n","\n","Showing Dataframe after adding unknowns\n","+-------+--------------+---------------+--------------+-------+\n","|User_ID|City          |State          |Country       |Age    |\n","+-------+--------------+---------------+--------------+-------+\n","|1      |nyc           |new york       |usa           |unknown|\n","|2      |stockton      |california     |usa           |18     |\n","|3      |moscow        |yukon territory|russia        |unknown|\n","|4      |porto         |vngaia         |portugal      |17     |\n","|5      |farnborough   |hants          |united kingdom|unknown|\n","|6      |santa monica  |california     |usa           |61     |\n","|7      |washington    |dc             |usa           |unknown|\n","|8      |timmins       |ontario        |canada        |unknown|\n","|9      |germantown    |tennessee      |usa           |unknown|\n","|10     |albacete      |wisconsin      |spain         |26     |\n","|11     |melbourne     |victoria       |australia     |14     |\n","|12     |fort bragg    |california     |usa           |unknown|\n","|13     |barcelona     |barcelona      |spain         |26     |\n","|14     |mediapolis    |iowa           |usa           |unknown|\n","|15     |calgary       |alberta        |canada        |unknown|\n","|16     |albuquerque   |new mexico     |usa           |unknown|\n","|17     |chesapeake    |virginia       |usa           |unknown|\n","|18     |rio de janeiro|rio de janeiro |brazil        |25     |\n","|19     |weston        |unknown        |unknown       |14     |\n","|20     |langhorne     |pennsylvania   |usa           |19     |\n","+-------+--------------+---------------+--------------+-------+\n","only showing top 20 rows\n","\n","+-------+-----------------+------+-------------+--------+-----------------+\n","|summary|User_ID          |City  |State        |Country |Age              |\n","+-------+-----------------+------+-------------+--------+-----------------+\n","|count  |278858           |278858|278858       |278858  |278858           |\n","|mean   |139429.5         |null  |null         |null    |34.81453247765849|\n","|stddev |80499.51502027822|null  |null         |null    |14.32252136949242|\n","|min    |1                |a     |a            |a       |1                |\n","|max    |99999            |zzzzz |zurich kanton|zimbabwe|unknown          |\n","+-------+-----------------+------+-------------+--------+-----------------+\n","\n","Null count after dealing with nulls\n","+-------+----+-----+-------+---+\n","|User_ID|City|State|Country|Age|\n","+-------+----+-----+-------+---+\n","|      0|   0|    0|      0|  0|\n","+-------+----+-----+-------+---+\n","\n","unknown count After replacing nulls to unknown\n","+-------+----+-----+-------+------+\n","|User_ID|City|State|Country|   Age|\n","+-------+----+-----+-------+------+\n","|      0|1317|17579|   4764|112128|\n","+-------+----+-----+-------+------+\n","\n","Creating DIM_USERS...........\n","Final look at DIM USERS\n","+------+------+--------------+---------------+--------------+-------+\n","|UserSK|UserBK|          City|          State|       Country|    Age|\n","+------+------+--------------+---------------+--------------+-------+\n","|   101|     1|           nyc|       new york|           usa|unknown|\n","|   102|     2|      stockton|     california|           usa|     18|\n","|   103|     3|        moscow|yukon territory|        russia|unknown|\n","|   104|     4|         porto|         vngaia|      portugal|     17|\n","|   105|     5|   farnborough|          hants|united kingdom|unknown|\n","|   106|     6|  santa monica|     california|           usa|     61|\n","|   107|     7|    washington|             dc|           usa|unknown|\n","|   108|     8|       timmins|        ontario|        canada|unknown|\n","|   109|     9|    germantown|      tennessee|           usa|unknown|\n","|   110|    10|      albacete|      wisconsin|         spain|     26|\n","|   111|    11|     melbourne|       victoria|     australia|     14|\n","|   112|    12|    fort bragg|     california|           usa|unknown|\n","|   113|    13|     barcelona|      barcelona|         spain|     26|\n","|   114|    14|    mediapolis|           iowa|           usa|unknown|\n","|   115|    15|       calgary|        alberta|        canada|unknown|\n","|   116|    16|   albuquerque|     new mexico|           usa|unknown|\n","|   117|    17|    chesapeake|       virginia|           usa|unknown|\n","|   118|    18|rio de janeiro| rio de janeiro|        brazil|     25|\n","|   119|    19|        weston|        unknown|       unknown|     14|\n","|   120|    20|     langhorne|   pennsylvania|           usa|     19|\n","+------+------+--------------+---------------+--------------+-------+\n","only showing top 20 rows\n","\n","+-------+-----------------+-----------------+------+-------------+--------+------------------+\n","|summary|           UserSK|           UserBK|  City|        State| Country|               Age|\n","+-------+-----------------+-----------------+------+-------------+--------+------------------+\n","|  count|           278858|           278858|278858|       278858|  278858|            278858|\n","|   mean|         139529.5|         139429.5|  null|         null|    null| 34.81453247765849|\n","| stddev|80499.51502027822|80499.51502027822|  null|         null|    null|14.322521369492517|\n","|    min|              101|                1|     a|            a|       a|                 1|\n","|    max|           278958|            99999| zzzzz|zurich kanton|zimbabwe|           unknown|\n","+-------+-----------------+-----------------+------+-------------+--------+------------------+\n","\n"]}],"source":["#Preparing Users Dataframe\n","\n","from pyspark.sql.functions import col,isnan,when,count\n","\n","#Splitting Columns\n","UsersDF_1 = Users_DF.withColumn('User_ID', f.split(Users_DF['UserstoSPLIT'], ';').getItem(0)) \\\n","       .withColumn('City', f.split(Users_DF['UserstoSPLIT'], ';').getItem(1)) \\\n","       .withColumn('Country', f.split(Users_DF['AgeSPLIT'], ';').getItem(0)) \\\n","       .withColumn('Age', f.split(Users_DF['AgeSPLIT'], ';').getItem(1))\n","\n","\n","#4000 rows of data were lost because they written on other columns (Columns D,E,F in CSV) with missing user ID's\n","\n","#Drop the column from before\n","UsersDF_1 = UsersDF_1.drop('UserstoSPLIT','AgeSPLIT')\n","\n","#Trimming leading and trailing spaces\n","\n","for C1 in UsersDF_1.columns:\n","  UsersDF_1 = UsersDF_1.withColumn(C1, trim(col(C1)))\n","\n","\n","#Important, countries and cities with & would become concatenated when junk characters are fixed\n","for C1 in UsersDF_1.columns:\n","  UsersDF_1 = UsersDF_1.withColumn(C1, f.regexp_replace(C1, ' & ', ' and '))\n","\n","#Tiding up the columns\n","UsersDF_1 = UsersDF_1.select('User_ID', 'City', 'State', 'Country', 'Age')\n","\n","#Getting rid of junk characters\n","\n","for C1 in UsersDF_1.columns:\n","  UsersDF_1 = UsersDF_1.withColumn(C1, f.regexp_replace(C1, '[^0-9a-zA-Z $]+', ''))\n","\n","for C1 in UsersDF_1.columns:\n","  UsersDF_1 = UsersDF_1.withColumn(C1, f.regexp_replace(C1, 'NULL', ''))\n","\n","#Trim Twice to be nice\n","for C1 in UsersDF_1.columns:\n","  UsersDF_1 = UsersDF_1.withColumn(C1, trim(col(C1)))\n","\n","print('UsersDF_1 before transformation')\n","UsersDF_1.show(truncate=False)\n","\n","#Finding out how many nulls in dataset\n","print('Null count before clean')\n","UsersDF_1.select([count(when(isnan(C1) |(col(C1) == 'NULL')  | (col(C1) == 'na') | (col(C1) == '') | col(C1).isNull(), C1)).alias(C1) for C1 in UsersDF_1.columns]\n","   ).show()\n","\n","#Getting rid of empty values and nulls\n","for C1 in UsersDF_1.columns:\n","  UsersDF_1 = UsersDF_1.withColumn(C1, when((col(C1) == '') | (col(C1) == 'na') | (col(C1) == 0) | \\\n","                                            (col(C1) == 'NULL') | (col(C1) == ' ') | (col(C1).isNull()), 'unknown').otherwise(col(C1)))\n","\n","#puting unknown on age above 250\n","UsersDF_1 = UsersDF_1.withColumn(C1, when((col('Age') > 250), 'unknown').otherwise(col('Age')))\n","\n","\n","#Small Fix remove first row with text columns\n","UsersDF_1 = UsersDF_1.where(col('User_ID') != 'UserID')\n","\n","#Data clening\n","UsersDF_1 = UsersDF_1.withColumn('City', when(~(col('City').rlike(\"^[a-zA-Z ]*$\")),'unknown').otherwise(col('City')))\n","UsersDF_1 = UsersDF_1.withColumn('State', when(~(col('State').rlike(\"^[a-zA-Z ]*$\")),'unknown').otherwise(col('State')))\n","UsersDF_1 = UsersDF_1.withColumn('Country', when(~(col('Country').rlike(\"^[a-zA-Z ]*$\")),'unknown').otherwise(col('Country')))\n","\n","\n","\n","#fix find cities with alternative forms that all refer to the same city\\country under one name,\n","UsersDF_1 = UsersDF_1.withColumn('City', when((col('City') == 'brighton') | (col('City') == 'brighton and hove') | (col('City') == 'hove') , 'brighton and hove').otherwise(col('City')))\n","UsersDF_1 = UsersDF_1.withColumn('City', when((col('City') == 'sandiego') | (col('City') == 'san diego'), 'san diego').otherwise(col('City')))\n","UsersDF_1 = UsersDF_1.withColumn('Country', when((col('Country') == 'brasil') | (col('Country') == 'brazil'), 'brazil').otherwise(col('Country')))\n","\n","#Removing Rows that dont have a userID\n","UsersDF_1 = UsersDF_1.filter(col(\"User_ID\") != 'unknown')\n","\n","print('Showing Dataframe after adding unknowns')\n","UsersDF_1.show(truncate=False)\n","\n","UsersDF_1.describe().show(truncate=False)\n","\n","\n","#Finding out how many nulls in dataset\n","#for some reason only works with import before code\n","print('Null count after dealing with nulls')\n","\n","UsersDF_1.select([count(when(isnan(C1) |(col(C1) == 'NULL') | col(C1).isNull(), C1)).alias(C1) for C1 in UsersDF_1.columns]\n","   ).show()\n","\n","#Finding out how many unknown in dataset\n","print('unknown count After replacing nulls to unknown')\n","UsersDF_1.select([count(when(col(C1) == 'unknown', C1)).alias(C1) for C1 in UsersDF_1.columns]\n","   ).show()\n","\n","#Creating DIM_Users\n","\n","print('Creating DIM_USERS...........')\n","\n","from pyspark.sql.functions import row_number, monotonically_increasing_id\n","from pyspark.sql import Window\n","\n","UsersDF_1 = UsersDF_1.withColumnRenamed('User_ID', 'UserBK') \\\n","\n","\n","UsersDF_1 = UsersDF_1.withColumn(\n","    \"UserSK\",\n","    row_number().over(Window.orderBy(monotonically_increasing_id()))+100\n",")\n","\n","UsersDF_1 = UsersDF_1.select('UserSK', 'UserBK', 'City', 'State', 'Country', 'Age')\n","\n","DIM_USERS = UsersDF_1\n","\n","\n","\n","\n","print('Final look at DIM USERS')\n","DIM_USERS.show()\n","DIM_USERS.describe().show()\n"]},{"cell_type":"markdown","metadata":{"id":"FOQGA2IHuzj4"},"source":["##4. Preparing Rating DataFrame\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"7hI8DwqgR4ep","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689785128841,"user_tz":-180,"elapsed":166301,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}},"outputId":"62e27d6d-2533-45fe-b95c-fbf0dd9d429d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Null count before before replacing nulls with column mean\n","+-------+----+-----------+\n","|User_ID|ISBN|Book_Rating|\n","+-------+----+-----------+\n","|      0|   0|          5|\n","+-------+----+-----------+\n","\n","Stats counting Alphabetic values\n","+-------+----+-----------+\n","|User_ID|ISBN|Book_Rating|\n","+-------+----+-----------+\n","|      0|  56|          0|\n","+-------+----+-----------+\n","\n","#########################################################################################################################\n","After Running our state of the art high tech algorythim, we now know how many bookshave valid and invalid ISBN's\n","\n","There are 1034693 Bookratings with Valid ISBN\n","There are 13882 Booksratings with Invalid ISBN\n","\n","\n","There should be 1034693 Booksratings with valid ISBN in the updated database\n","\n","Here is a count of the total rows in the updated BookRating dataframe\n","\n","#####  1034693  #####\n","\n","quick look at the cleaned dataset\n","+-------+----------+-----------+\n","|User_ID|      ISBN|Book_Rating|\n","+-------+----------+-----------+\n","| 276725|034545104X|          0|\n","| 276726|0155061224|          5|\n","| 276727|0446520802|          0|\n","| 276729|052165615X|          3|\n","| 276729|0521795028|          6|\n","| 276733|2080674722|          0|\n","| 276736|3257224281|          8|\n","| 276737|0600570967|          6|\n","| 276744|038550120X|          7|\n","| 276746|0425115801|          0|\n","| 276746|0449006522|          0|\n","| 276746|0553561618|          0|\n","| 276746|055356451X|          0|\n","| 276746|0786013990|          0|\n","| 276746|0786014512|          0|\n","| 276747|0060517794|          9|\n","| 276747|0451192001|          0|\n","| 276747|0609801279|          0|\n","| 276747|0671537458|          9|\n","| 276747|0679776818|          8|\n","+-------+----------+-----------+\n","only showing top 20 rows\n","\n","Creating FACT_RATING...........\n","Final look at Fact Table\n","+--------+------+------+------+\n","|RatingSK|BookSK|UserSK|Rating|\n","+--------+------+------+------+\n","|694433  |254254|171218|8     |\n","|352552  |215824|86223 |0     |\n","|854707  |215824|209616|0     |\n","|102229  |42651 |24002 |9     |\n","|797389  |112611|196249|0     |\n","|841859  |146237|206400|0     |\n","|102231  |42404 |24002 |9     |\n","|1008391 |227844|245094|0     |\n","|1019270 |227433|246771|0     |\n","|58656   |12863 |12044 |9     |\n","|102232  |12863 |24002 |9     |\n","|102233  |42542 |24002 |8     |\n","|450571  |42542 |110001|8     |\n","|102234  |42574 |24002 |0     |\n","|229704  |249632|53829 |0     |\n","|317396  |199133|76726 |0     |\n","|773420  |152908|189935|5     |\n","|773421  |152936|189935|5     |\n","|349738  |213939|85626 |8     |\n","|378936  |122142|93147 |8     |\n","+--------+------+------+------+\n","only showing top 20 rows\n","\n","+-------+-----------------+-----------------+------------------+------------------+\n","|summary|RatingSK         |BookSK           |UserSK            |Rating            |\n","+-------+-----------------+-----------------+------------------+------------------+\n","|count  |863933           |863933           |863933            |863933            |\n","|mean   |518740.5586579052|68326.21752265512|128892.12181384437|2.8531112945101067|\n","|stddev |298862.9033490375|72752.93090363982|74237.81502390353 |3.8580924002543284|\n","|min    |102              |101              |102               |0                 |\n","|max    |1034793          |270963           |278954            |9                 |\n","+-------+-----------------+-----------------+------------------+------------------+\n","\n"]}],"source":["\n","#Cleaning BookRating DataFrame\n","\n","from pyspark.sql.functions import col,isnan,when,count\n","#Splitting Columns\n","BookRatingsDF_1 = BookRatings_DF.withColumn('User_ID', f.split(BookRatings_DF['\"User-ID;\"\"ISBN\"\";\"\"Book-Rating\"\"\"'], ';').getItem(0)) \\\n","       .withColumn('ISBN', f.split(BookRatings_DF['\"User-ID;\"\"ISBN\"\";\"\"Book-Rating\"\"\"'], ';').getItem(1)) \\\n","       .withColumn('Book_Rating', f.split(BookRatings_DF['\"User-ID;\"\"ISBN\"\";\"\"Book-Rating\"\"\"'], ';').getItem(2))\n","\n","#Dropping Old Column\n","BookRatingsDF_1 = BookRatingsDF_1.drop('\"User-ID;\"\"ISBN\"\";\"\"Book-Rating\"\"\"', '_c1', '_c2')\n","\n","#Getting rid of junk characters\n","for C1 in BookRatingsDF_1.columns:\n","  BookRatingsDF_1 = BookRatingsDF_1.withColumn(C1, f.regexp_replace(C1, '[^0-9a-zA-Z$]+', ''))\n","\n","#Trimming\n","for C1 in BookRatingsDF_1.columns:\n","  BookRatingsDF_1 = BookRatingsDF_1.withColumn(C1, trim(col(C1)))\n","\n","#Finding out how many nulls in dataset\n","print('Null count before before replacing nulls with column mean')\n","BookRatingsDF_1.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in BookRatingsDF_1.columns]\n","   ).show()\n","\n","#Nulls only in 'Book Rating' Column, also checked unique values to see rating is from 0-10 only\n","#Replacing nulls in 'Book Rating'\n","avgdf = BookRatingsDF_1.agg({'Book_Rating': 'mean'})\n","avgdf = avgdf.withColumn(\"avg(Book_Rating)\",avgdf[\"avg(Book_Rating)\"].cast(StringType()))\n","avg = avgdf.rdd.map(lambda x: x[0]).collect()\n","BookRatingsDF_1 = BookRatingsDF_1.fillna(avg[0], subset=['Book_Rating'])\n","\n","\n","#Checikng if any aplphabetic-only values in strictly numeric columns, in this case all of them.\n","print('Stats counting Alphabetic values')\n","BookRatingsDF_1.select([count(when(col(c).rlike(\"^[a-zA-Z]*$\"), c)).alias(c) for c in BookRatingsDF_1.columns]\n","   ).show()\n","\n","\n","\n","####################################################################################################################################################\n","\n","# ISBN VALIDITY SCREANING\n","\n","####################################################################################################################################################\n","\n","ValidISBN = []\n","InvalidISBN = []\n","\n","row_listBOOKRATINGS = BookRatingsDF_1.collect()\n","\n","for i in row_listBOOKRATINGS:\n","  if (is_isbn10(i.__getitem__('ISBN')) == True):\n","    ValidISBN.append(i)\n","  else:\n","    InvalidISBN.append(i)\n","print('#########################################################################################################################')\n","print(\"After Running our state of the art high tech algorythim, we now know how many bookshave valid and invalid ISBN's\")\n","print('')\n","print(f'There are {len(ValidISBN)} Bookratings with Valid ISBN')\n","print(f'There are {len(InvalidISBN)} Booksratings with Invalid ISBN')\n","\n","InvalidISBN_ONLY = []\n","for i in InvalidISBN:\n","  InvalidISBN_ONLY.append(i.__getitem__('ISBN'))\n","\n","BookRatingsDF_1 = BookRatingsDF_1.filter(~col('ISBN').isin(InvalidISBN_ONLY))\n","print('')\n","print('')\n","print(f'There should be {len(ValidISBN)} Booksratings with valid ISBN in the updated database')\n","print('')\n","\n","print('Here is a count of the total rows in the updated BookRating dataframe')\n","print('')\n","print('#####  ' + f'{BookRatingsDF_1.count()}' + '  #####' )\n","print('')\n","\n","\n","\n","print('quick look at the cleaned dataset')\n","BookRatingsDF_1.show()\n","\n","\n","#CREATING FACT_Rating\n","print('Creating FACT_RATING...........')\n","BookRatingsDF_1 = BookRatingsDF_1.withColumnRenamed('User_ID', 'UserBK') \\\n",".withColumnRenamed('ISBN', 'BookBK') \\\n",".withColumnRenamed('Book_Rating', 'Rating')\n","\n","from pyspark.sql.functions import row_number, monotonically_increasing_id\n","from pyspark.sql import Window\n","\n","BookRatingsDF_1 = BookRatingsDF_1.withColumn(\n","    \"RatingSK\",\n","    row_number().over(Window.orderBy(monotonically_increasing_id()))+100\n",")\n","\n","BookRatingsDF_1 = BookRatingsDF_1.select('RatingSK', 'UserBK', 'BookBK', 'Rating')\n","\n","\n","#Final FACT TABLE, joining table\n","UsersDF_1.createOrReplaceTempView(\"Users\")\n","BooksDF_1.createOrReplaceTempView(\"Books\")\n","BookRatingsDF_1.createOrReplaceTempView(\"Ratings\")\n","\n","FACT_RATING =spark.sql('''\n","Select RatingSK, BookSK, UserSK, Rating FROM Ratings R inner join\n","Users U on R.UserBK = U.UserBK inner join Books B on R.BookBK = B.BookBK\n","'''\n",")\n","#Info on Fact Table\n","\n","print('Final look at Fact Table')\n","FACT_RATING.show(truncate=False)\n","FACT_RATING.describe().show(truncate=False)\n"]},{"cell_type":"markdown","metadata":{"id":"tjmBSY2YujPS"},"source":["##5. Writing the completed DIMS and FACT as CSV's\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"bsDrCbn_uh0S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689785219302,"user_tz":-180,"elapsed":90473,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}},"outputId":"a3fc2508-2490-4fdc-c307-8d23d7209883"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files are ready and waiting in /content/drive/Othercomputers/Ami computer /Ami/Technion/4.big data/Big Data Project/data\n"]}],"source":["DIM_USERS.write.option(\"header\",True).mode(\"overwrite\").csv(f'{FolderPath}/dim_users')\n","DIM_BOOKS.write.option(\"header\",True).mode(\"overwrite\").csv(f'{FolderPath}/dim_books')\n","FACT_RATING.write.option(\"header\",True).mode(\"overwrite\").csv(f'{FolderPath}/fact_rating')\n","\n","print(f'Files are ready and waiting in {FolderPath}')"]},{"cell_type":"markdown","metadata":{"id":"Higd-n55vLQk"},"source":["##6. Extras"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"IlHHES97uuZD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689785225985,"user_tz":-180,"elapsed":6697,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}},"outputId":"b9ef8448-05b9-49fa-df29-3cf611a26dbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+-------------------+\n","|Book_Title|Year_Of_Publication|\n","+----------+-------------------+\n","+----------+-------------------+\n","\n"]}],"source":["#SQL Query to check other things too- here to check all years under 1000 do not show up in dataframe\n","\n","\n","BooksDF_1.createOrReplaceTempView(\"Books\")\n","Query =spark.sql('''\n","Select Book_Title, Year_Of_Publication FROM Books\n","WHERE Year_Of_Publication < 1000\n","Order By Year_Of_Publication desc\n","'''\n",")\n","\n","Query.show(truncate=False)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"GnmNIIaSvPfL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689785236779,"user_tz":-180,"elapsed":10803,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}},"outputId":"ac601b29-263b-49ae-96aa-f0ccf3deb561"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+------+------+------+\n","|RatingSK|UserBK|BookBK|Rating|\n","+--------+------+------+------+\n","+--------+------+------+------+\n","\n"]}],"source":["#Just A Simple Filter to Check things\n","\n","\n","SimpleFilter= BookRatingsDF_1.filter( \\\n","    col(\"User_ID\") == '130499' \\\n","  )\n","\n","\n","\n","SimpleFilter.show(truncate=False)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ThQuJflxvYCZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689785261363,"user_tz":-180,"elapsed":24595,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}},"outputId":"a310433c-3496-4f76-c935-cafc953f81ed"},"outputs":[{"output_type":"stream","name":"stdout","text":[" 1 users rows deleted \n","117 Books rows deleted\n","13882 BookRatings rows deleted\n"]}],"source":["#Checking how much data was lost\n","\n","print(f\" {Users_DF.count()-UsersDF_1.count()} users rows deleted \")\n","print(f\"{Books_DF.count()-BooksDF_1.count()} Books rows deleted\")\n","print(f\"{BookRatings_DF.count()-BookRatingsDF_1.count()} BookRatings rows deleted\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"7IatLzs4_4bY","executionInfo":{"status":"ok","timestamp":1689785261363,"user_tz":-180,"elapsed":5,"user":{"displayName":"Ami Sharabi","userId":"14925774216131589133"}}},"execution_count":15,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}