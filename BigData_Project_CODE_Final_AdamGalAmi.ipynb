{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdH6RLqfewQg"
   },
   "source": [
    "Big Data Project- Adam, Ami, Gal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghn33L2pvsKO"
   },
   "source": [
    "# Contents \n",
    "\n",
    "1. Connecting to Spark Agent & to drive in order to uplaod files, creating pathways for files\n",
    "\n",
    "2. Preparing Books Dataframe & creating the DIM version\n",
    "\n",
    "3. preparing Users Dataframe & creating the DIM version\n",
    "\n",
    "4. Preparing Rating Dataframe & creating FACT version\n",
    "\n",
    "5. Writing new DIMS & FACT to CSV format\n",
    "\n",
    "6. Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j76kQs1nwlmP"
   },
   "source": [
    "##1. Connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THMjq5OPepMG",
    "outputId": "7e36eda0-55da-4b3f-df38-db63d19131c3"
   },
   "outputs": [],
   "source": [
    "\n",
    "!apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null   # !apt-get --> install java\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
    "!tar xf spark-3.3.0-bin-hadoop3.tgz \n",
    "!pip install -q findspark \n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init(\"spark-3.3.0-bin-hadoop3\") \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAlyZzubfSCy",
    "outputId": "8861f5e5-3004-4777-d800-0f7dd5b81c49"
   },
   "outputs": [],
   "source": [
    "#Installing a library that confirms an ISBN as valid\n",
    "\n",
    "!pip install isbnlib\n",
    "\n",
    "from isbnlib import is_isbn10, is_isbn13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7ArRPcjfxtg",
    "outputId": "42537583-703a-4ff8-fb4c-908876a557d3"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Connecting to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e49jepQpgGud"
   },
   "outputs": [],
   "source": [
    "#Setting Up personal Folder Paths' Data Files reside (Remove '#' where relevant)\n",
    "\n",
    "#Adam's File location\n",
    "FolderPath = '/content/drive/MyDrive/Big Data Project'\n",
    "\n",
    "#Ami's File location: \n",
    "#FolderPath =  '/content/drive/Othercomputers/Ami computer /Ami/טכניון/big data/Big Data Project/data'\n",
    "\n",
    "#Gal's File location: \n",
    "#FolderPath = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVUonM5VhNGf"
   },
   "outputs": [],
   "source": [
    "# reading files\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "#uploaded in standard format\n",
    "BookRatings_DF = spark.read.csv(FolderPath + '/BX-Book-Ratings.csv', header = True, inferSchema= True)\n",
    "\n",
    "#Best way to upload file without losing data, parsing via a delimiter before reading   \n",
    "Books_DF = (spark.read.option(\"delimiter\", ';\"').option(\"header\", True).format(\"csv\").load(FolderPath + '/BX-Books.csv'))\n",
    "\n",
    "#Users CSV is missing headers, creating schema with headers before the read\n",
    "schema = StructType([\n",
    "    StructField(\"UserstoSPLIT\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"AgeSPLIT\", StringType(), True)])\n",
    "\n",
    "Users_DF = spark.read.csv(FolderPath + '/BX-Users.csv', header = True, schema = schema) # Using schema made above \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwJoZdOliEJQ"
   },
   "outputs": [],
   "source": [
    "#Lets Have a look at the mess\n",
    "BookRatings_DF.show(truncate = False)\n",
    "Books_DF.show()\n",
    "Users_DF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ko7T8rSvl1z"
   },
   "source": [
    "##2. Preparing Book Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyOxpBLB6wo5",
    "outputId": "297d14a3-8853-4af2-f491-3f6e47faf596"
   },
   "outputs": [],
   "source": [
    "#Cleaning Books DataFrame\n",
    "\n",
    "#More Libraries\n",
    "import re\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "\n",
    "\n",
    "# IMPORTANT, special character '&' created a delimiter in wrong place, this little fix saved 5000+ rows of data!\n",
    "\n",
    "for C1 in Books_DF.columns:\n",
    "  BooksDF_1 = Books_DF.withColumn(C1, f.regexp_replace(C1, '&amp;', 'and'))\n",
    "\n",
    "\n",
    "\n",
    "#Getting rid of junk characters\n",
    "for C1 in BooksDF_1.columns:\n",
    "  BooksDF_1 = BooksDF_1.withColumn(C1, f.regexp_replace(C1, '[^0-9a-zA-Z $]+', ''))\n",
    "\n",
    "BooksDF_1 = BooksDF_1.select([f.col(col).alias(re.sub(\"-\",\"_\",col)) for col in BooksDF_1.columns])\n",
    "\n",
    "BooksDF_1 = BooksDF_1.select([f.col(col).alias(re.sub(\"[^0-9a-zA-Z_$]+\",\"\",col)) for col in BooksDF_1.columns])\n",
    "\n",
    "#Trimming\n",
    "from pyspark.sql.functions import trim\n",
    "for C1 in BooksDF_1.columns:\n",
    "  BooksDF_1 = BooksDF_1.withColumn(C1, trim(col(C1)))\n",
    "\n",
    "#Getting rid of empty values\n",
    "for c1 in BooksDF_1.columns:\n",
    "  BooksDF_1 = BooksDF_1.withColumn(c1, when(col(c1) == '', 'unknown').otherwise(col(c1)))\n",
    "\n",
    "#Finding out how many nulls in dataset\n",
    "#for some reason only works with import before code\n",
    "print('Null Count Before Clean')\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "BooksDF_1.select([count(when(isnan(C1) | col(C1).isNull(), C1)).alias(C1) for C1 in BooksDF_1.columns]\n",
    "   ).show()\n",
    "\n",
    "#Fill Nulls with 'Unkown' because values are qualitative \n",
    "BooksDF_1 = BooksDF_1.fillna('Unknown')\n",
    "\n",
    "#No Nulls\n",
    "print('Null Count after Clean')\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "BooksDF_1.select([count(when(isnan(C1) | col(C1).isNull(), C1)).alias(C1) for C1 in BooksDF_1.columns]\n",
    "   ).show()\n",
    "\n",
    "#Lowecase All\n",
    "for C1 in BooksDF_1.columns:\n",
    "    BooksDF_1 = BooksDF_1.withColumn(C1, f.lower(f.col(C1)))\n",
    "\n",
    "\n",
    "#Check Nulls are filled with unknown\n",
    "print('After filling empty values as unknown, we count the database again')\n",
    "print('Unknown Count')\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "BooksDF_1.select([count(when(col(C1)== 'unknown', C1)).alias(C1) for C1 in BooksDF_1.columns]\n",
    "   ).show()\n",
    "\n",
    "BooksforFilter = BooksDF_1 \n",
    "\n",
    "# 4000 rows have '0' as publish year\n",
    "Filter= BooksforFilter.filter( \\\n",
    "    col(\"Year_Of_Publication\") < 1000 \\\n",
    "  ) \n",
    "print('Count of Years with improbable values 0-1000')\n",
    "Filter.describe().show(truncate=False)\n",
    "\n",
    "#Turning unusual years to unknown \n",
    "BooksDF_1 = BooksDF_1.withColumn('Year_Of_Publication', when((col('Year_Of_Publication') < 200) | \\\n",
    "                                                              (~col('Year_Of_Publication').rlike(\"^[0-9]*$\")), 'unknown') \\\n",
    "                                                             .otherwise(col('Year_Of_Publication')))\n",
    "                                                          \n",
    "#Columns which have only numeric values but should be limited to alphabetic, changed to 'unknown'\n",
    "for C1 in BooksDF_1.columns:\n",
    "  if C1 not in ('ISBN', 'Year_Of_Publication'):\n",
    "    BooksDF_1 = BooksDF_1.withColumn(C1, when(col(C1).rlike(\"^[0-9]*$\"), 'unknown').otherwise(col(C1)))\n",
    "\n",
    "\n",
    "####################################################################################################################################################\n",
    "\n",
    "# ISBN VALIDITY SCREANING\n",
    "\n",
    "####################################################################################################################################################\n",
    "\n",
    "ValidISBN = []\n",
    "InvalidISBN = []\n",
    "\n",
    "row_listBOOKS = BooksDF_1.collect()\n",
    "\n",
    "for i in row_listBOOKS:\n",
    "  if (is_isbn10(i.__getitem__('ISBN')) == True):\n",
    "    ValidISBN.append(i)\n",
    "  else:\n",
    "    InvalidISBN.append(i) \n",
    "print('#########################################################################################################################')\n",
    "print(\"After Running our state of the art high tech algorythim, we now know how many bookshave valid and invalid ISBN's\")\n",
    "print('')\n",
    "print(f'There are {len(ValidISBN)} Books with Valid ISBN')\n",
    "print(f'There are {len(InvalidISBN)} Books with Invalid ISBN')\n",
    "\n",
    "InvalidISBN_ONLY = []\n",
    "for i in InvalidISBN:\n",
    "  InvalidISBN_ONLY.append(i.__getitem__('ISBN')) \n",
    "\n",
    "BooksDF_1 = BooksDF_1.filter(~col('ISBN').isin(InvalidISBN_ONLY))\n",
    "print('')\n",
    "print('')\n",
    "print(f'There should be {len(ValidISBN)} books with valid ISBN in the updated database')\n",
    "print('')\n",
    "\n",
    "print('Here is a count of the total rows in the updated Books dataframe')\n",
    "print('')\n",
    "print('#####  ' + f'{BooksDF_1.count()}' + '  #####' )\n",
    "\n",
    "#CREATING DIM_BOOKS\n",
    "\n",
    "print('Creating DIM_BOOKS...........')\n",
    "\n",
    "BooksDF_1 = BooksDF_1.drop('Image_URL_S','Image_URL_M','Image_URL_L')\n",
    "\n",
    "BooksDF_1 = BooksDF_1.withColumnRenamed('ISBN', 'BookBK')\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql import Window\n",
    "\n",
    "BooksDF_1 = BooksDF_1.withColumn(\n",
    "    \"BookSK\",\n",
    "    row_number().over(Window.orderBy(monotonically_increasing_id()))+100\n",
    ")\n",
    "\n",
    "BooksDF_1 = BooksDF_1.select('BookSK', 'BookBK', 'Book_Title', 'Book_Author', 'Year_Of_Publication', 'Publisher')\n",
    "\n",
    "DIM_BOOKS = BooksDF_1\n",
    "#Se Tout\n",
    "print('Final Look at the DIM')\n",
    "\n",
    "\n",
    "DIM_BOOKS.show()\n",
    "DIM_BOOKS.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPLppLe_u-xn"
   },
   "source": [
    "##3. Preparing Users Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wB-rJFVW7R9t",
    "outputId": "eaf362f0-c156-4e80-9316-28edaa893efc"
   },
   "outputs": [],
   "source": [
    "#Preparing Users Dataframe\n",
    "\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "#Splitting Columns\n",
    "UsersDF_1 = Users_DF.withColumn('User_ID', f.split(Users_DF['UserstoSPLIT'], ';').getItem(0)) \\\n",
    "       .withColumn('City', f.split(Users_DF['UserstoSPLIT'], ';').getItem(1)) \\\n",
    "       .withColumn('Country', f.split(Users_DF['AgeSPLIT'], ';').getItem(0)) \\\n",
    "       .withColumn('Age', f.split(Users_DF['AgeSPLIT'], ';').getItem(1))\n",
    "\n",
    "\n",
    "#4000 rows of data were lost because they written on other columns (Columns D,E,F in CSV) with missing user ID's\n",
    "\n",
    "#Drop the column from before\n",
    "UsersDF_1 = UsersDF_1.drop('UserstoSPLIT','AgeSPLIT')\n",
    "\n",
    "#Trimming leading and trailing spaces \n",
    "\n",
    "for C1 in UsersDF_1.columns:\n",
    "  UsersDF_1 = UsersDF_1.withColumn(C1, trim(col(C1)))\n",
    "\n",
    "\n",
    "#Important, countries and cities with & would become concatenated when junk characters are fixed\n",
    "for C1 in UsersDF_1.columns:\n",
    "  UsersDF_1 = UsersDF_1.withColumn(C1, f.regexp_replace(C1, ' & ', ' and '))\n",
    "\n",
    "#Tiding up the columns\n",
    "UsersDF_1 = UsersDF_1.select('User_ID', 'City', 'State', 'Country', 'Age')\n",
    "\n",
    "#Getting rid of junk characters\n",
    "\n",
    "for C1 in UsersDF_1.columns:\n",
    "  UsersDF_1 = UsersDF_1.withColumn(C1, f.regexp_replace(C1, '[^0-9a-zA-Z $]+', ''))\n",
    "\n",
    "for C1 in UsersDF_1.columns:\n",
    "  UsersDF_1 = UsersDF_1.withColumn(C1, f.regexp_replace(C1, 'NULL', ''))\n",
    "\n",
    "#Trim Twice to be nice\n",
    "for C1 in UsersDF_1.columns:\n",
    "  UsersDF_1 = UsersDF_1.withColumn(C1, trim(col(C1)))\n",
    "\n",
    "print('UsersDF_1 before transformation')\n",
    "UsersDF_1.show(truncate=False)\n",
    "\n",
    "#Finding out how many nulls in dataset\n",
    "print('Null count before clean')\n",
    "UsersDF_1.select([count(when(isnan(C1) |(col(C1) == 'NULL')  | (col(C1) == 'na') | (col(C1) == '') | col(C1).isNull(), C1)).alias(C1) for C1 in UsersDF_1.columns]\n",
    "   ).show()\n",
    "\n",
    "#Getting rid of empty values and nulls\n",
    "for C1 in UsersDF_1.columns:\n",
    "  UsersDF_1 = UsersDF_1.withColumn(C1, when((col(C1) == '') | (col(C1) == 'na') | (col(C1) == 0) | \\\n",
    "                                            (col(C1) == 'NULL') | (col(C1) == ' ') | (col(C1).isNull()), 'unknown').otherwise(col(C1)))\n",
    "\n",
    "#puting unknown on age above 250\n",
    "UsersDF_1 = UsersDF_1.withColumn(C1, when((col('Age') > 250), 'unknown').otherwise(col('Age')))\n",
    "  \n",
    "\n",
    "#Small Fix remove first row with text columns\n",
    "UsersDF_1 = UsersDF_1.where(col('User_ID') != 'UserID')\n",
    "\n",
    "#Data clening\n",
    "UsersDF_1 = UsersDF_1.withColumn('City', when(~(col('City').rlike(\"^[a-zA-Z ]*$\")),'unknown').otherwise(col('City')))\n",
    "UsersDF_1 = UsersDF_1.withColumn('State', when(~(col('State').rlike(\"^[a-zA-Z ]*$\")),'unknown').otherwise(col('State')))\n",
    "UsersDF_1 = UsersDF_1.withColumn('Country', when(~(col('Country').rlike(\"^[a-zA-Z ]*$\")),'unknown').otherwise(col('Country')))\n",
    "\n",
    "\n",
    "\n",
    "#fix find cities with alternative forms that all refer to the same city\\country under one name, \n",
    "UsersDF_1 = UsersDF_1.withColumn('City', when((col('City') == 'brighton') | (col('City') == 'brighton and hove') | (col('City') == 'hove') , 'brighton and hove').otherwise(col('City')))\n",
    "UsersDF_1 = UsersDF_1.withColumn('City', when((col('City') == 'sandiego') | (col('City') == 'san diego'), 'san diego').otherwise(col('City')))\n",
    "UsersDF_1 = UsersDF_1.withColumn('Country', when((col('Country') == 'brasil') | (col('Country') == 'brazil'), 'brazil').otherwise(col('Country')))\n",
    "\n",
    "#Removing Rows that dont have a userID\n",
    "UsersDF_1 = UsersDF_1.filter(col(\"User_ID\") != 'unknown') \n",
    "\n",
    "print('Showing Dataframe after adding unknowns')\n",
    "UsersDF_1.show(truncate=False)\n",
    "\n",
    "UsersDF_1.describe().show(truncate=False)\n",
    "\n",
    "\n",
    "#Finding out how many nulls in dataset\n",
    "#for some reason only works with import before code\n",
    "print('Null count after dealing with nulls')\n",
    "\n",
    "UsersDF_1.select([count(when(isnan(C1) |(col(C1) == 'NULL') | col(C1).isNull(), C1)).alias(C1) for C1 in UsersDF_1.columns]\n",
    "   ).show()\n",
    "\n",
    "#Finding out how many unknown in dataset\n",
    "print('unknown count After replacing nulls to unknown')\n",
    "UsersDF_1.select([count(when(col(C1) == 'unknown', C1)).alias(C1) for C1 in UsersDF_1.columns]\n",
    "   ).show()   \n",
    "\n",
    "#Creating DIM_Users\n",
    "\n",
    "print('Creating DIM_USERS...........')\n",
    "\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql import Window\n",
    "\n",
    "UsersDF_1 = UsersDF_1.withColumnRenamed('User_ID', 'UserBK') \\\n",
    "\n",
    "\n",
    "UsersDF_1 = UsersDF_1.withColumn(\n",
    "    \"UserSK\",\n",
    "    row_number().over(Window.orderBy(monotonically_increasing_id()))+100\n",
    ")\n",
    "\n",
    "UsersDF_1 = UsersDF_1.select('UserSK', 'UserBK', 'City', 'State', 'Country', 'Age')\n",
    "\n",
    "DIM_USERS = UsersDF_1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Final look at DIM USERS')\n",
    "DIM_USERS.show()\n",
    "DIM_USERS.describe().show()  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOQGA2IHuzj4"
   },
   "source": [
    "##4. Preparing Rating DataFrame \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hI8DwqgR4ep",
    "outputId": "66796c7c-a92e-400d-d587-2dd480ae5af5"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Cleaning BookRating DataFrame\n",
    "\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "#Splitting Columns\n",
    "BookRatingsDF_1 = BookRatings_DF.withColumn('User_ID', f.split(BookRatings_DF['\"User-ID;\"\"ISBN\"\";\"\"Book-Rating\"\"\"'], ';').getItem(0)) \\\n",
    "       .withColumn('ISBN', f.split(BookRatings_DF['\"User-ID;\"\"ISBN\"\";\"\"Book-Rating\"\"\"'], ';').getItem(1)) \\\n",
    "       .withColumn('Book_Rating', f.split(BookRatings_DF['\"User-ID;\"\"ISBN\"\";\"\"Book-Rating\"\"\"'], ';').getItem(2))\n",
    "\n",
    "#Dropping Old Column\n",
    "BookRatingsDF_1 = BookRatingsDF_1.drop('\"User-ID;\"\"ISBN\"\";\"\"Book-Rating\"\"\"', '_c1', '_c2') \n",
    "\n",
    "#Getting rid of junk characters\n",
    "for C1 in BookRatingsDF_1.columns:\n",
    "  BookRatingsDF_1 = BookRatingsDF_1.withColumn(C1, f.regexp_replace(C1, '[^0-9a-zA-Z$]+', ''))\n",
    "\n",
    "#Trimming\n",
    "for C1 in BookRatingsDF_1.columns:\n",
    "  BookRatingsDF_1 = BookRatingsDF_1.withColumn(C1, trim(col(C1)))\n",
    "\n",
    "#Finding out how many nulls in dataset\n",
    "print('Null count before before replacing nulls with column mean')\n",
    "BookRatingsDF_1.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in BookRatingsDF_1.columns]\n",
    "   ).show()\n",
    "\n",
    "#Nulls only in 'Book Rating' Column, also checked unique values to see rating is from 0-10 only\n",
    "#Replacing nulls in 'Book Rating'\n",
    "avgdf = BookRatingsDF_1.agg({'Book_Rating': 'mean'})\n",
    "avgdf = avgdf.withColumn(\"avg(Book_Rating)\",avgdf[\"avg(Book_Rating)\"].cast(StringType()))\n",
    "avg = avgdf.rdd.map(lambda x: x[0]).collect()\n",
    "BookRatingsDF_1 = BookRatingsDF_1.fillna(avg[0], subset=['Book_Rating'])\n",
    "\n",
    "\n",
    "#Checikng if any aplphabetic-only values in strictly numeric columns, in this case all of them. \n",
    "print('Stats counting Alphabetic values')\n",
    "BookRatingsDF_1.select([count(when(col(c).rlike(\"^[a-zA-Z]*$\"), c)).alias(c) for c in BookRatingsDF_1.columns]\n",
    "   ).show()\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################################################################\n",
    "\n",
    "# ISBN VALIDITY SCREANING\n",
    "\n",
    "####################################################################################################################################################\n",
    "\n",
    "ValidISBN = []\n",
    "InvalidISBN = []\n",
    "\n",
    "row_listBOOKRATINGS = BookRatingsDF_1.collect()\n",
    "\n",
    "for i in row_listBOOKRATINGS:\n",
    "  if (is_isbn10(i.__getitem__('ISBN')) == True):\n",
    "    ValidISBN.append(i)\n",
    "  else:\n",
    "    InvalidISBN.append(i) \n",
    "print('#########################################################################################################################')\n",
    "print(\"After Running our state of the art high tech algorythim, we now know how many bookshave valid and invalid ISBN's\")\n",
    "print('')\n",
    "print(f'There are {len(ValidISBN)} Bookratings with Valid ISBN')\n",
    "print(f'There are {len(InvalidISBN)} Booksratings with Invalid ISBN')\n",
    "\n",
    "InvalidISBN_ONLY = []\n",
    "for i in InvalidISBN:\n",
    "  InvalidISBN_ONLY.append(i.__getitem__('ISBN')) \n",
    "\n",
    "BookRatingsDF_1 = BookRatingsDF_1.filter(~col('ISBN').isin(InvalidISBN_ONLY))\n",
    "print('')\n",
    "print('')\n",
    "print(f'There should be {len(ValidISBN)} Booksratings with valid ISBN in the updated database')\n",
    "print('')\n",
    "\n",
    "print('Here is a count of the total rows in the updated BookRating dataframe')\n",
    "print('')\n",
    "print('#####  ' + f'{BookRatingsDF_1.count()}' + '  #####' )\n",
    "print('')\n",
    "\n",
    "\n",
    "\n",
    "print('quick look at the cleaned dataset')\n",
    "BookRatingsDF_1.show()\n",
    "\n",
    "\n",
    "#CREATING FACT_Rating\n",
    "print('Creating FACT_RATING...........')\n",
    "BookRatingsDF_1 = BookRatingsDF_1.withColumnRenamed('User_ID', 'UserBK') \\\n",
    ".withColumnRenamed('ISBN', 'BookBK') \\\n",
    ".withColumnRenamed('Book_Rating', 'Rating') \n",
    "\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql import Window\n",
    "\n",
    "BookRatingsDF_1 = BookRatingsDF_1.withColumn(\n",
    "    \"RatingSK\",\n",
    "    row_number().over(Window.orderBy(monotonically_increasing_id()))+100\n",
    ")\n",
    "\n",
    "BookRatingsDF_1 = BookRatingsDF_1.select('RatingSK', 'UserBK', 'BookBK', 'Rating')\n",
    "\n",
    "\n",
    "#Final FACT TABLE, joining table\n",
    "UsersDF_1.createOrReplaceTempView(\"Users\")\n",
    "BooksDF_1.createOrReplaceTempView(\"Books\")\n",
    "BookRatingsDF_1.createOrReplaceTempView(\"Ratings\")\n",
    "\n",
    "FACT_RATING =spark.sql('''\n",
    "Select RatingSK, BookSK, UserSK, Rating FROM Ratings R inner join \n",
    "Users U on R.UserBK = U.UserBK inner join Books B on R.BookBK = B.BookBK \n",
    "'''\n",
    ")\n",
    "#Info on Fact Table\n",
    "\n",
    "print('Final look at Fact Table')\n",
    "FACT_RATING.show(truncate=False)\n",
    "FACT_RATING.describe().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjmBSY2YujPS"
   },
   "source": [
    "##5. Writing the completed DIMS and FACT as CSV's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsDrCbn_uh0S"
   },
   "outputs": [],
   "source": [
    "DIM_USERS.write.option(\"header\",True).mode(\"overwrite\").csv(f'{FolderPath}/dim_users')\n",
    "DIM_BOOKS.write.option(\"header\",True).mode(\"overwrite\").csv(f'{FolderPath}/dim_books')\n",
    "FACT_RATING.write.option(\"header\",True).mode(\"overwrite\").csv(f'{FolderPath}/fact_rating')\n",
    "\n",
    "print(f'Files are ready and waiting in {FolderPath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Higd-n55vLQk"
   },
   "source": [
    "##6. Extras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlHHES97uuZD"
   },
   "outputs": [],
   "source": [
    "#SQL Query to check other things too- here to check all years under 1000 do not show up in dataframe\n",
    "\n",
    "\n",
    "BooksDF_1.createOrReplaceTempView(\"Books\")\n",
    "Query =spark.sql('''\n",
    "Select Book_Title, Year_Of_Publication FROM Books\n",
    "WHERE Year_Of_Publication < 1000\n",
    "Order By Year_Of_Publication desc\n",
    "'''\n",
    ")\n",
    "\n",
    "Query.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnmNIIaSvPfL"
   },
   "outputs": [],
   "source": [
    "#Just A Simple Filter to Check things\n",
    "\n",
    "\n",
    "SimpleFilter= BookRatingsDF_1.filter( \\\n",
    "    col(\"User_ID\") == '130499' \\\n",
    "  ) \n",
    "\n",
    "\n",
    "\n",
    "SimpleFilter.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThQuJflxvYCZ",
    "outputId": "441c0ff1-f204-4e9f-a243-ea0444319656"
   },
   "outputs": [],
   "source": [
    "#Checking how much data was lost\n",
    "\n",
    "print(f\" {Users_DF.count()-UsersDF_1.count()} users rows deleted \")\n",
    "print(f\"{Books_DF.count()-BooksDF_1.count()} Books rows deleted\")\n",
    "print(f\"{BookRatings_DF.count()-BookRatingsDF_1.count()} BookRatings rows deleted\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
